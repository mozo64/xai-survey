{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: alibi in ./.local/lib/python3.8/site-packages (0.6.5)\r\n",
      "Collecting scikit-learn-extra\r\n",
      "  Downloading scikit_learn_extra-0.2.0-cp38-cp38-manylinux2010_x86_64.whl (1.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: transformers<5.0.0,>=4.7.0 in ./.local/lib/python3.8/site-packages (from alibi) (4.18.0)\r\n",
      "Requirement already satisfied: scikit-learn<1.1.0,>=0.20.2 in ./.local/lib/python3.8/site-packages (from alibi) (0.24.2)\r\n",
      "Requirement already satisfied: pandas<2.0.0,>=0.23.3 in ./.local/lib/python3.8/site-packages (from alibi) (1.3.2)\r\n",
      "Requirement already satisfied: Pillow<10.0,>=5.4.1 in ./.local/lib/python3.8/site-packages (from alibi) (9.0.1)\r\n",
      "Requirement already satisfied: spacy[lookups]<4.0.0,>=2.0.0 in ./.local/lib/python3.8/site-packages (from alibi) (3.1.2)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.16.2 in ./.local/lib/python3.8/site-packages (from alibi) (1.21.0)\r\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.1.0 in ./.local/lib/python3.8/site-packages (from alibi) (1.7.1)\r\n",
      "Requirement already satisfied: dill<0.4.0,>=0.3.0 in ./.local/lib/python3.8/site-packages (from alibi) (0.3.4)\r\n",
      "Requirement already satisfied: scikit-image!=0.17.1,<0.20,>=0.14.2 in ./.local/lib/python3.8/site-packages (from alibi) (0.19.2)\r\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.0.0 in ./.local/lib/python3.8/site-packages (from alibi) (3.4.3)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/lib/python3/dist-packages (from alibi) (2.22.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.8/site-packages (from alibi) (3.10.0.2)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.28.1 in ./.local/lib/python3.8/site-packages (from alibi) (4.62.2)\r\n",
      "Requirement already satisfied: attrs<22.0.0,>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from alibi) (21.2.0)\r\n",
      "Requirement already satisfied: tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0 in ./.local/lib/python3.8/site-packages (from alibi) (2.8.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.8/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (2.7.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.local/lib/python3.8/site-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (1.3.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib<4.0.0,>=3.0.0->alibi) (2.4.7)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas<2.0.0,>=0.23.3->alibi) (2019.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (21.0)\r\n",
      "Requirement already satisfied: networkx>=2.2 in ./.local/lib/python3.8/site-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (2.7.1)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in ./.local/lib/python3.8/site-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (2022.3.16)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in ./.local/lib/python3.8/site-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (1.3.0)\r\n",
      "Requirement already satisfied: imageio>=2.4.1 in ./.local/lib/python3.8/site-packages (from scikit-image!=0.17.1,<0.20,>=0.14.2->alibi) (2.16.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.8/site-packages (from scikit-learn<1.1.0,>=0.20.2->alibi) (2.2.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in ./.local/lib/python3.8/site-packages (from scikit-learn<1.1.0,>=0.20.2->alibi) (1.0.1)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (2.0.5)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.0.5)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.6.0)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.8 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (8.0.9)\r\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.3.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.0.1)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.7 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (3.0.8)\r\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (45.2.0)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.7.4)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.5)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (2.4.1)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (0.8.2)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.4 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (2.0.6)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.8.2)\r\n",
      "Requirement already satisfied: spacy-lookups-data<1.1.0,>=1.0.2 in ./.local/lib/python3.8/site-packages (from spacy[lookups]<4.0.0,>=2.0.0->alibi) (1.0.3)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.1.2)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.25.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.2.0)\r\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (2.8.0.dev2021122109)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.1.0)\r\n",
      "Requirement already satisfied: flatbuffers>=1.12 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (2.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.6.3)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.14.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.44.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.14.0)\r\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (2.8.0)\r\n",
      "Requirement already satisfied: protobuf>=3.9.2 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.20.1)\r\n",
      "Requirement already satisfied: libclang>=9.0.1 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (14.0.1)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.6.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4.0 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.0.0)\r\n",
      "Requirement already satisfied: gast>=0.2.1 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.5.3)\r\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (2.8.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.local/lib/python3.8/site-packages (from tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.3.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.7.0->alibi) (0.12.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.7.0->alibi) (5.3.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.7.0->alibi) (3.0.12)\r\n",
      "Requirement already satisfied: sacremoses in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.7.0->alibi) (0.0.49)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.7.0->alibi) (2022.3.15)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in ./.local/lib/python3.8/site-packages (from transformers<5.0.0,>=4.7.0->alibi) (0.5.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.34.2)\r\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in ./.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy[lookups]<4.0.0,>=2.0.0->alibi) (5.2.1)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in ./.local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.4.6)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in ./.local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (2.6.6)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in ./.local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.8.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.3.6)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in ./.local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (2.1.2)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in ./.local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.6.1)\r\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in ./.local/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy[lookups]<4.0.0,>=2.0.0->alibi) (7.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy[lookups]<4.0.0,>=2.0.0->alibi) (2.0.1)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (4.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (5.0.0)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (4.11.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in ./.local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.8.0)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (0.4.8)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow!=2.6.0,!=2.6.1,<2.9.0,>=2.0.0->alibi) (3.1.0)\r\n",
      "Installing collected packages: scikit-learn-extra\r\n",
      "Successfully installed scikit-learn-extra-0.2.0\r\n"
     ]
    }
   ],
   "source": [
    "! pip3 install alibi scikit-learn-extra"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0658d8f7-f7b2-44a4-967f-bc219f0d4a5e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import socket\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.cluster\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score, classification_report\n",
    "from alibi.explainers import AnchorTabular, AnchorText\n",
    "from sklearn.neighbors import KDTree, NearestCentroid\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import spacy\n",
    "model_spacy = 'en_core_web_lg'\n",
    "spacy_nlp = spacy.load(model_spacy)\n",
    "\n",
    "\n",
    "class ModelGlobalExplainer():\n",
    "  def __init__(self, ):\n",
    "    pass\n",
    "\n",
    "  def fit(self, x, y, point_no_to_global):\n",
    "    self.explainer = DecisionTreeClassifier(random_state=0,\n",
    "                                            max_depth=math.floor(math.log2(point_no_to_global)))\n",
    "    self.explainer.fit(x, y)\n",
    "    pass\n",
    "\n",
    "  def predict(self,\n",
    "      X_test,\n",
    "      features  # keep to be consistent with modellocalexplainer\n",
    "  ):\n",
    "    self.y_pred = self.explainer.predict(X_test)\n",
    "    pass\n",
    "\n",
    "\n",
    "class ModelLocalExplainer():\n",
    "  def __init__(self,\n",
    "      clf: sklearn.base.BaseEstimator,\n",
    "      class_names: list):\n",
    "    self.clf = clf\n",
    "    self.class_names = class_names\n",
    "    pass\n",
    "\n",
    "  def fit(self,\n",
    "      dataset: pd.DataFrame,  #dataset that will be used to train the explainer\n",
    "      features: list,\n",
    "      thresh: float\n",
    "      #target:pd.Series # column with target values\n",
    "  ):\n",
    "\n",
    "    explainer = self._fit_explainer(dataset.iloc[:, :-1], features)\n",
    "    self.rules = self._justify_explainer(dataset, explainer, thresh)\n",
    "    self.hmr_file_name = self.df2hmr(data=self.rules.drop('mult', axis=1), df_columns_names=features)\n",
    "    pass\n",
    "\n",
    "  def predict(self, X_test, features):\n",
    "    self.y_pred = self._heartdroid(X_test, features)\n",
    "    pass\n",
    "\n",
    "  ###HMR file creation\n",
    "  def inparse(self, condition):\n",
    "    fs = re.sub(r'([-+]?[0-9]+\\.[0-9]+)(<=|>=|<|>|)(f[0-9]+)(<=|>=|<|>)([-+]?[0-9]+\\.[0-9]+)', r'\\2', condition)\n",
    "    ss = re.sub(r'([-+]?[0-9]+\\.[0-9]+)(<=|>=|<|>|)(f[0-9]+)(<=|>=|<|>)([-+]?[0-9]+\\.[0-9]+)', r'\\4', condition)\n",
    "    res = None\n",
    "    if fs == '<':\n",
    "      val = re.sub(r'([-+]?[0-9]+\\.[0-9]+)(<=|>=|<|>|)(f[0-9]+)(<=|>=|<|>)([-+]?[0-9]+\\.[0-9]+)', r'\\1', condition)\n",
    "      res = re.sub(r'([-+]?[0-9]+\\.[0-9]+)(<=|>=|<|>|)(f[0-9]+)(<=|>=|<|>)([-+]?[0-9]+\\.[0-9]+)', r'\\3 in [' + str(eval(val) + 0.001) + r' to \\5 ]', condition)\n",
    "    if ss == '<':\n",
    "      val = re.sub(r'([-+]?[0-9]+\\.[0-9]+)(<=|>=|<|>|)(f[0-9]+)(<=|>=|<|>)([-+]?[0-9]+\\.[0-9]+)', r'\\5', condition)\n",
    "      res = re.sub(r'([-+]?[0-9]+\\.[0-9]+)(<=|>=|<|>|)(f[0-9]+)(<=|>=|<|>)([-+]?[0-9]+\\.[0-9]+)', r'\\3 in [\\1 to ' + str(eval(val) - 0.001) + ']', condition)\n",
    "    if res is None:\n",
    "      return condition\n",
    "    else:\n",
    "      return res\n",
    "\n",
    "  def tohmr(self, series):\n",
    "    result = []\n",
    "    for v in series.split('AND'):\n",
    "      v = self.inparse(v.strip().lower().replace(' ', ''))\n",
    "      result.append(v.replace('<=', ' lte ')\n",
    "                    .replace('>=', ' gte ').replace('<', ' lt ').replace('>', ' gt ').replace('=', 'eq').lower())\n",
    "    return '[' + ','.join(result) + ']'\n",
    "\n",
    "  def df2hmr(self, data, df_columns_names):\n",
    "    numfeats = len(df_columns_names)\n",
    "\n",
    "    types = \"\"\"xtype [name: float,\n",
    "    domain: [-10000 to 10000],\n",
    "    scale: 0,\n",
    "    base: numeric\n",
    "    ].\n",
    "xtype [name: clustertype,\n",
    "    domain: [0 to 1000],\n",
    "    scale: 0,\n",
    "    base: numeric\n",
    "    ].\n",
    "\"\"\"\n",
    "    atts_cluster = \"\"\"\n",
    "xattr [name: cluster,\n",
    "    type: clustertype,\n",
    "    class: simple,\n",
    "    comm: out\n",
    "    ].\n",
    "\"\"\"\n",
    "    atts_placeholder = \"\"\"\n",
    "xattr [name: __NAME__,\n",
    "    type: float,\n",
    "    class: simple,\n",
    "    comm: out\n",
    "    ].\n",
    "\"\"\"\n",
    "    schema_placeholder = \"\"\"xschm anchor: [__NAME__] ==> [cluster].\n",
    "\"\"\"\n",
    "    data['hmr_cond'] = data['Rule'].apply(self.tohmr)\n",
    "    data['confidence'] = data['Coverage'] * data['Precision']\n",
    "    atts = ''\n",
    "    schemacond = []\n",
    "    #for i in range(1,numfeats+1):\n",
    "    #    atts+=atts_placeholder.replace('__NAME__','f'+str(i))\n",
    "    #    schemacond.append('f'+str(i))\n",
    "    for i in df_columns_names:\n",
    "      atts += atts_placeholder.replace('__NAME__', i)\n",
    "      schemacond.append(i)\n",
    "\n",
    "      #print(schemacond)\n",
    "    schema = schema_placeholder.replace('__NAME__', ','.join(schemacond))\n",
    "\n",
    "    #print(schema)\n",
    "    \"\"\"\n",
    "    WITH open('model.hmr','w') AS f:\n",
    "        f.write(types)\n",
    "        f.write(atts)\n",
    "        f.write(atts_cluster)\n",
    "        f.write(SCHEMA)\n",
    "        FOR i,r IN DATA.iterrows():\n",
    "            f.write('xrule anchor/'+str(i)+': '+r['hmr_cond']+ ' ==>  [cluster set '+str(r['Cluster'])+']. #'+str(r['confidence'])+'\\n')\n",
    "    \"\"\"\n",
    "    f = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.hmr', dir=os.getcwd())\n",
    "    #config_path = f'{f.name}.hmr'\n",
    "    #with (config_path,'w') as f:\n",
    "    f.write(types)\n",
    "    f.write(atts)\n",
    "    f.write(atts_cluster)\n",
    "    f.write(schema)\n",
    "    for i, r in data.iterrows():\n",
    "      f.write('xrule anchor/' + str(i) + ': ' + r['hmr_cond'] + ' ==>  [cluster set ' + str(r['Cluster']) + ']. #' + str(r['confidence']) + '\\n')\n",
    "    f.close()\n",
    "    #print(f.name)\n",
    "    return f.name.split('/')[-1]\n",
    "\n",
    "  def _heartdroid(self, X_test_con, features):\n",
    "    temp_list = []\n",
    "    X_test = X_test_con.values\n",
    "    model = self.hmr_file_name\n",
    "\n",
    "    print('Heartdroid run')\n",
    "\n",
    "    for steps in tqdm(range(X_test.shape[0]), desc=\"Loading...\", position=0, leave=True):\n",
    "      finall_string = ''\n",
    "      for it, index in enumerate(features):\n",
    "        finall_string += f' -A {index}={X_test[steps][it]}'\n",
    "\n",
    "      #output_list = !java -jar HMRCommandLine.jar {model} -tabs anchor{finall_string}\n",
    "      output_list = queryHRTDServer(f'{model} -tabs anchor{finall_string}')\n",
    "      output_list = output_list.split('\\n')\n",
    "      print(output_list)\n",
    "\n",
    "      for o in reversed(output_list):\n",
    "        if 'Attribute cluster' in o:\n",
    "          output_list = [o]\n",
    "\n",
    "      if 'null' in output_list[0]: ## Fixed\n",
    "        temp_list.append(-1)  #undefined cluster\n",
    "      else:\n",
    "        temp_list.append(int(float(output_list[0].split(\" = \")[-1])))\n",
    "    temp_list = np.array(temp_list)\n",
    "    return temp_list\n",
    "\n",
    "\n",
    "class ModelLocalExplainer_anchor(ModelLocalExplainer):\n",
    "\n",
    "  def _fit_explainer(self, dataset, features):\n",
    "    predict_fn = lambda x: self.clf.predict_proba(x)\n",
    "    explainer = AnchorTabular(predict_fn, features)\n",
    "    explainer.fit(dataset.values, disc_perc=(25, 50, 75))\n",
    "    return explainer\n",
    "\n",
    "  def _justify_explainer(self, dataset, explainer, thresh):\n",
    "    rules_out_list = []\n",
    "    for cluster in self.class_names:\n",
    "      rules_out = pd.DataFrame()\n",
    "      tempo_dataset = dataset[dataset['y'] == cluster]\n",
    "      for idx in range(tempo_dataset.shape[0]):\n",
    "        if self.class_names[explainer.predictor(tempo_dataset.iloc[:, :-1].values[idx].reshape(1, -1))[0]] == cluster:\n",
    "          explanation = explainer.explain(tempo_dataset.iloc[:, :-1].values[idx], threshold=thresh)\n",
    "          exp = explanation.anchor\n",
    "          rules_out = rules_out.append({'Rule': (' AND '.join(exp)),\n",
    "                                        'Precision': explanation['precision'],\n",
    "                                        'Coverage': explanation['coverage'],\n",
    "                                        'Cluster': cluster},\n",
    "                                       ignore_index=True)\n",
    "      rules_out['mult'] = rules_out['Precision'] * rules_out['Coverage']\n",
    "      rules_out_list.append(rules_out.sort_values('mult', ascending=False).drop_duplicates(subset=['Rule']).reset_index(drop=True))\n",
    "\n",
    "    rules_output = pd.concat(rules_out_list)\n",
    "    rules_output.reset_index(drop=True, inplace=True)\n",
    "    return rules_output\n",
    "\n",
    "\n",
    "class ModelLocalExplainer_anchor_text(ModelLocalExplainer):\n",
    "\n",
    "  def _fit_explainer(self, dataset, features):\n",
    "    predict_fn = lambda x: self.clf.predict_proba(x)\n",
    "    explainer = AnchorText(spacy_nlp, predict_fn)  ## TODO, features\n",
    "    # explainer.fit(dataset.values, disc_perc=(25, 50, 75))\n",
    "    return explainer\n",
    "\n",
    "  def _justify_explainer(self, dataset, explainer, thresh):\n",
    "    rules_out_list = []\n",
    "    for cluster in self.class_names:\n",
    "      rules_out = pd.DataFrame()\n",
    "      tempo_dataset = dataset[dataset['y'] == cluster]\n",
    "      for idx in range(tempo_dataset.shape[0]):\n",
    "        if self.class_names[explainer.predictor(tempo_dataset.iloc[:, :-1].values[idx].reshape(1, -1))[0]] == cluster:\n",
    "          explanation = explainer.explain(tempo_dataset.iloc[:, :-1].values[idx], threshold=thresh)\n",
    "          exp = explanation.anchor\n",
    "          rules_out = rules_out.append({'Rule': (' AND '.join(exp)),\n",
    "                                        'Precision': explanation['precision'],\n",
    "                                        'Coverage': explanation['coverage'],\n",
    "                                        'Cluster': cluster},\n",
    "                                       ignore_index=True)\n",
    "      rules_out['mult'] = rules_out['Precision'] * rules_out['Coverage']\n",
    "      rules_out_list.append(rules_out.sort_values('mult', ascending=False).drop_duplicates(subset=['Rule']).reset_index(drop=True))\n",
    "\n",
    "    rules_output = pd.concat(rules_out_list)\n",
    "    rules_output.reset_index(drop=True, inplace=True)\n",
    "    return rules_output\n",
    "\n",
    "\n",
    "class CLAMP(BaseEstimator):\n",
    "  def __init__(self,\n",
    "      bounding_box_selection: str = 'random',\n",
    "      classification_model: sklearn.base.BaseEstimator = LogisticRegression(),\n",
    "      clusterng_algorithm: sklearn.base.BaseEstimator = KMeans(),\n",
    "      description_points_ratio: float = 0.1,\n",
    "      test_size: float = 0.2,\n",
    "      metric: str = 'minkowski',\n",
    "      explainer_type: str = 'anchor',\n",
    "      thresh: float = 0.9,\n",
    "      conv_method: str = None\n",
    "  ):\n",
    "\n",
    "    self.bounding_box_selection = bounding_box_selection\n",
    "    self.classification_model = classification_model\n",
    "    self.clusterng_algorithm = clusterng_algorithm\n",
    "    self.description_points_ratio = description_points_ratio\n",
    "    self.test_size = test_size\n",
    "    self.metric = metric\n",
    "    self.explainer_type = explainer_type\n",
    "    self.thresh = thresh\n",
    "    self.hrd_accuracy = 0\n",
    "    self.conv_method = None\n",
    "\n",
    "    pass\n",
    "\n",
    "  def fit(self,\n",
    "      x_in: pd.DataFrame,  #data which will be used to train explainer model and classifier\n",
    "      y: pd.Series = None,  # cluster labels (not used, left for consistency with BaseEstimator)\n",
    "  ):\n",
    "    \"\"\"\n",
    "    #fits the Clustering algorithm and classifier\n",
    "    \"\"\"\n",
    "\n",
    "    #exchange column names\n",
    "    x, self.orignal_features = self._convert_features(x_in)\n",
    "    if y is None:\n",
    "      #clustering stage\n",
    "      y = self._clustering(x)  # only if y not in data\n",
    "      print('Data without labels, clustering stage implementation')\n",
    "    else:\n",
    "      y = np.array(y)\n",
    "      print('Data labeled')\n",
    "\n",
    "    #classification stage\n",
    "    self.X_train, self.X_test, self.y_train, self.y_test = self._recognize_input(x, y)\n",
    "    self.X_train = self._convert_to_norm(self.X_train, self.conv_method)\n",
    "    self.X_test = self._convert_to_norm(self.X_test, self.conv_method)\n",
    "    self.y_pred_clf, self.clf_model = self._classification()\n",
    "    self.clf_precision, self.clf_recall, self.clf_f1, self.clf_accuracy, self.clf_classification_report = self._scores(self.y_test, self.y_pred_clf)\n",
    "\n",
    "    #bounding box stage\n",
    "    self.df_model_input = self._bounding_box_method(self.X_train, self.y_train)\n",
    "    #print(math.floor(math.log2(self.point_no_to_global)+1))\n",
    "\n",
    "    if self.explainer_type == 'anchor':\n",
    "      self.explainer = ModelLocalExplainer_anchor(clf=self.clf_model, class_names=self.class_names)\n",
    "      print('Anchor tabular explainer')\n",
    "      self.explainer.fit(dataset=self.df_model_input, features=self.features, thresh=self.thresh)\n",
    "\n",
    "    if self.explainer_type == 'anchor_text':  ## TODO\n",
    "      self.explainer = ModelLocalExplainer_anchor_text(clf=self.clf_model, class_names=self.class_names)\n",
    "      print('Anchor text explainer')\n",
    "      self.explainer.fit(dataset=self.df_model_input, features=self.features, thresh=self.thresh)\n",
    "\n",
    "    elif self.explainer_type == 'global':\n",
    "      self.explainer = ModelGlobalExplainer()\n",
    "      print('DT explainer')\n",
    "      self.explainer.fit(x=self.X_train, y=self.y_train, point_no_to_global=self.point_no_to_global)\n",
    "    else:\n",
    "      ValueError('Explainer type not implemented. Select one of: anchor, global.')\n",
    "    pass\n",
    "\n",
    "  def justify(self):  #, X_test, y_test):\n",
    "    #self.explainer.justify(X_test, y_test)\n",
    "    #self.results = self._convert_rules(self.explainer.rules.drop('mult', axis = 1), self.orignal_features, self.features)\n",
    "    return self._convert_rules(self.explainer.rules.drop('mult', axis=1), self.orignal_features, self.features)\n",
    "\n",
    "  def predict(self, X_test, y_test):\n",
    "    self.explainer.predict(X_test, self.features)\n",
    "    self.y_pred_explainer = self.explainer.y_pred\n",
    "    self.explainer_precision, self.explainer_recall, self.explainer_f1, self.explainer_accuracy, self.explainer_classification_report = self._scores(y_test,\n",
    "                                                                                                                                                     self.y_pred_explainer)\n",
    "    pass\n",
    "\n",
    "  def _convert_rules(self, rules, org, mod):\n",
    "    my_dict = {}\n",
    "    for key in mod:\n",
    "      for value in org:\n",
    "        my_dict[key] = value\n",
    "        org.remove(value)\n",
    "        break\n",
    "    rules_con_org_features = []\n",
    "    for rule in rules['Rule']:\n",
    "      temp = rule.split()\n",
    "      res = []\n",
    "      for wrd in temp:\n",
    "        res.append(my_dict.get(wrd, wrd))\n",
    "      rules_con_org_features.append(' '.join(res))\n",
    "    rules['Rule'] = rules_con_org_features\n",
    "    return rules\n",
    "\n",
    "  def _convert_features(self, data):\n",
    "    orignal_features = list(data.columns)\n",
    "    data.columns = ['f' + str(list(data.columns).index(x)) for x in list(data.columns)]\n",
    "    return data, orignal_features\n",
    "\n",
    "  def _clustering(self, x):\n",
    "    try:\n",
    "      clustering_model = self.clusterng_algorithm.fit(x)\n",
    "      prediction = clustering_model.predict(x)\n",
    "    except:\n",
    "      prediction = self.clusterng_algorithm.fit_predict(x)\n",
    "    return prediction\n",
    "\n",
    "  def _recognize_input(self, x, y):\n",
    "    self.features = x.columns\n",
    "    self.num_of_features = len(self.features)\n",
    "    self.class_names = np.unique(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=self.test_size, random_state=0)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "  ####not used in grid search\n",
    "  def _convert_to_norm(self, x, method):\n",
    "    if method == 'standard_scaler':\n",
    "      scaler = StandardScaler()\n",
    "      scaler.fit(x)\n",
    "      converted_data = scaler.transform(x)\n",
    "    elif method == 'minmax_scaler':\n",
    "      scaler = MinMaxScaler()\n",
    "      scaler.fit(x)\n",
    "      converted_data = scaler.transform(x)\n",
    "    elif method == None:\n",
    "      converted_data = x\n",
    "    return converted_data\n",
    "\n",
    "  def _classification(self):\n",
    "    classification_model = self.classification_model.fit(self.X_train, self.y_train)\n",
    "    return classification_model.predict(self.X_test), classification_model\n",
    "\n",
    "  def _scores(self, y_test, y_pred):\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, labels=self.class_names)\n",
    "    return precision, recall, f1, accuracy, classification_rep  #, auc\n",
    "\n",
    "  def _bounding_box_method(self, x, y):\n",
    "    data = pd.concat([x.reset_index(drop=True), pd.Series(y)], axis=1)\n",
    "    data.columns = list(self.features) + ['y']\n",
    "    number_of_points = math.ceil(data.shape[0] * self.description_points_ratio)\n",
    "    self.point_no_to_global = number_of_points\n",
    "    #print(data.shape)\n",
    "    temp_list = []\n",
    "    if self.bounding_box_selection == 'random':\n",
    "      for cluster in self.class_names:\n",
    "        X_t = data[data['y'] == cluster]\n",
    "        try:\n",
    "          temp_list.append(X_t.sample(n=number_of_points))\n",
    "        except:\n",
    "          temp_list.append(X_t.sample(n=X_t.shape[0]))\n",
    "\n",
    "    elif self.bounding_box_selection == 'tree_query':\n",
    "      clf = NearestCentroid()\n",
    "      clf.fit(data.drop('y', axis=1).to_numpy(), data['y'].to_numpy())\n",
    "      df_centroids = pd.DataFrame(clf.centroids_, columns=self.features)\n",
    "      for cluster in self.class_names:\n",
    "        X_t = data[data['y'] == cluster].drop('y', axis=1).values\n",
    "        tree = KDTree(X_t, leaf_size=10, metric=self.metric)\n",
    "        dist, ind = tree.query(df_centroids.iloc[cluster].values.reshape(1, -1), k=len(X_t))\n",
    "        temp_df = data[data['y'] == cluster].iloc[ind[0][-number_of_points:], :]\n",
    "        temp_list.append(temp_df)\n",
    "\n",
    "    elif self.bounding_box_selection == 'outliers':\n",
    "      for cluster in self.class_names:\n",
    "        cluster_cont = number_of_points / (data[data['y'] == cluster].shape[0])\n",
    "        if cluster_cont > 0.5:\n",
    "          print(\"number of description points cannot be higher than 50%, value has been changed to maximum\")\n",
    "          cluster_cont = 0.5\n",
    "        random_data = data[data['y'] == cluster].drop('y', axis=1).values\n",
    "        clf = IsolationForest(random_state=0, contamination=cluster_cont)\n",
    "        preds = clf.fit_predict(random_data)\n",
    "        df_temp = pd.DataFrame(random_data[[i for i, x in enumerate(preds) if x == -1]], columns=self.features)\n",
    "        df_temp['y'] = [cluster] * len(df_temp)\n",
    "        temp_list.append(df_temp)\n",
    "\n",
    "    elif self.bounding_box_selection == 'centroids':\n",
    "      for cluster in self.class_names:\n",
    "        X_t = data[data['y'] == cluster].drop('y', axis=1)\n",
    "        X_t = X_t.to_numpy()\n",
    "        kmedoids = KMedoids(n_clusters=1, random_state=0).fit(X_t)\n",
    "\n",
    "        df_temp = pd.DataFrame(data=kmedoids.cluster_centers_, columns=data.drop('y', axis=1).columns)\n",
    "        df_temp['y'] = [cluster] * len(df_temp)\n",
    "        temp_list.append(df_temp)\n",
    "    else:\n",
    "      ValueError('Bounding box method not implemented. Select one of: random, tree_query, outliers.')\n",
    "    df_ready = pd.concat(temp_list)\n",
    "    df_ready.reset_index(inplace=True, drop=True)\n",
    "    return df_ready\n",
    "\n",
    "  def _create_necessary_dir(self):\n",
    "    path = pathlib.Path().resolve() / 'hmr_models'\n",
    "    isExist = os.path.exists(path)\n",
    "    if not isExist:\n",
    "      os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2f6aec4-285d-45b0-9d38-d1d3b7e29f66",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In console run:\n",
    "java -jar HMRServer.jar <numer_portu> <ilosc_watkow> e.g.\n",
    "java -jar HMRServer.jar 9999 24\n",
    "'''\n",
    "\n",
    "HOST = \"localhost\"  #\"127.0.0.1\"  # The server's hostname or IP address\n",
    "PORT = 30017 # The port used by the server # 30017\n",
    "\n",
    "\n",
    "def queryHRTDServer(query, max_msg_size=1024):\n",
    "  query += '\\n'\n",
    "  with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.connect((HOST, PORT))\n",
    "    s.sendall(bytes(query, 'UTF8'))\n",
    "    fragments = []\n",
    "    while True:\n",
    "      chunk = s.recv(max_msg_size)\n",
    "      if not chunk:\n",
    "        break\n",
    "      fragments.append(chunk)\n",
    "    arr = b''.join(fragments)\n",
    "  return arr.decode('UTF8')\n",
    "\n",
    "\n",
    "def rem_hmr_files():\n",
    "  for x in os.listdir():\n",
    "    if x.endswith(\".hmr\"):\n",
    "      os.remove(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c4084-11a7-4406-a8b3-16ecb7d5feef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Example based on iris dataset without crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "046c1818-6a3b-4455-81cf-ed65981dfeb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'iris.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2aac1da5-6935-45cc-8b21-0bd10c273609",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('notebooks/xai-survey/art_dataset/' + dataset)\n",
    "labels = data['y']\n",
    "data = data.drop('y', axis=1)\n",
    "\n",
    "# labels were removed because one of the clamp's feature is to make clustering, \n",
    "# however if necessary you can pass labels in fit function and then clustring stage will be omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd0f1356-9775-4ffa-8cf4-83732d9afda8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         f0        f1        f2        f3\n0 -0.900681  1.019004 -1.340227 -1.315444\n1 -1.143017 -0.131979 -1.340227 -1.315444\n2 -1.385353  0.328414 -1.397064 -1.315444\n3 -1.506521  0.098217 -1.283389 -1.315444\n4 -1.021849  1.249201 -1.340227 -1.315444",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>f0</th>\n      <th>f1</th>\n      <th>f2</th>\n      <th>f3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.900681</td>\n      <td>1.019004</td>\n      <td>-1.340227</td>\n      <td>-1.315444</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.143017</td>\n      <td>-0.131979</td>\n      <td>-1.340227</td>\n      <td>-1.315444</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.385353</td>\n      <td>0.328414</td>\n      <td>-1.397064</td>\n      <td>-1.315444</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.506521</td>\n      <td>0.098217</td>\n      <td>-1.283389</td>\n      <td>-1.315444</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.021849</td>\n      <td>1.249201</td>\n      <td>-1.340227</td>\n      <td>-1.315444</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ef1081e-60de-4a63-aa54-8a32df585641",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clamp = CLAMP(clusterng_algorithm=KMeans(n_clusters=3),  # TODO\n",
    "              classification_model=xgb.XGBClassifier(),\n",
    "              description_points_ratio=0.01,\n",
    "              test_size=0.2,\n",
    "              metric='minkowski',\n",
    "              thresh=0.9,\n",
    "              bounding_box_selection='random')\n",
    "#parameters to adjust there is also possibility to change clustering algorithm and classification model\n",
    "# available bounding_box_selection parameter: centroids, outliers, tree_query, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee099294-97f4-4fc8-b7fb-3c6949bd2830",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data without labels, clustering stage implementation\n",
      "[08:11:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Anchor tabular explainer\n"
     ]
    }
   ],
   "source": [
    "#clamp.fit(data, labels) -- this one is for data with labels\n",
    "clamp.fit(data)  # -- this one is for data without labels\n",
    "\n",
    "#the dataset has been splited in to train and test dataset, train dataset in provided to anchor to generate rules based on boundingbox method and test dataset is saved to run predict method (check below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7e03e9a-0b64-4077-84ac-4d1284c54fe7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heartdroid run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading...: 100%|██████████| 30/30 [00:00<00:00, 346.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n",
      "['ERROR: null', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predict = clamp.predict(clamp.X_test, clamp.y_test)  #labels generation based on the test dataset in generated rules in previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bc6e216-848c-4549-bcea-b7d0e7ff7ec2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(clamp.explainer_precision)\n",
    "#score calculation (comparison labels from test dataset and those predicted by the clamp -- previous step)\n",
    "#available: explainer_f1, explainer_accuracy, explainer_classification_report, explainer_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "4cf5c3a9-ec87-4c21-929e-eb1d7f442183",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rule</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 &gt; 0.50 AND 0 &gt; 0.83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 &lt;= 0.50 AND 0 &gt; 0.83</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 &lt;= -0.94 AND 3 &lt;= -1.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 &gt; -0.94 AND 0 &lt;= 0.83 AND 1 &lt;= -0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 &gt; 0.42 AND 3 &lt;= 1.22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Rule  Precision  Coverage  Cluster\n",
       "0                   1 > 0.50 AND 0 > 0.83        1.0  0.166667        0\n",
       "1                  1 <= 0.50 AND 0 > 0.83        1.0  0.166667        0\n",
       "2               2 <= -0.94 AND 3 <= -1.02        1.0  0.333333        1\n",
       "3  2 > -0.94 AND 0 <= 0.83 AND 1 <= -0.25        1.0  0.333333        2\n",
       "4                  2 > 0.42 AND 3 <= 1.22        1.0  0.166667        2"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = clamp.justify()\n",
    "r\n",
    "rem_hmr_files()  # remove temp hmr files\n",
    "#display genarated rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217ba411-266b-465b-b0fb-9a9127e09250",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "8da4f110-0b83-4d20-854e-756c9d2c1423",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "8c507fe1-4a58-4481-b612-b6b28d4eadab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scorer(clamp, *args):\n",
    "  clamp.predict(clamp.X_test, clamp.y_test)\n",
    "  return {'f1': clamp.explainer_f1, 'accuracy': clamp.explainer_accuracy, 'precision': clamp.explainer_precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "79d9fb6f-281c-4198-8e50-1d0803744c99",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameters = [{\n",
    "  'bounding_box_selection': ['random', 'centroids', 'outliers', 'tree_query'],\n",
    "  'description_points_ratio': [0.05, 0.1, 0.2],\n",
    "  'test_size': [0.2],\n",
    "  'thresh': [0.9],\n",
    "  'explainer_type': ['anchor']\n",
    "},\n",
    "\n",
    "  {\n",
    "    'explainer_type': ['global'],\n",
    "    'test_size': [0.2]\n",
    "  }]\n",
    "\n",
    "list_of_choosen_datasets = ['iris.csv']\n",
    "list_of_clusters = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "4895ea34-c666-45cc-8c19-421110289649",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: iris.csv\n",
      "Data without labels, clustering stage implementation\n",
      "Anchor explainer\n",
      "Heartdroid run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading...: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:00<00:00, 160.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data without labels, clustering stage implementation\n",
      "Anchor explainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% % time\n",
    "#parameter settings as described above\n",
    "cv_restuls = []\n",
    "cv_datasets = []\n",
    "cv_clf = []\n",
    "\n",
    "for dataset, cluster_number in zip(list_of_choosen_datasets, list_of_clusters):\n",
    "  data = pd.read_csv('art_datasets/' + dataset)\n",
    "\n",
    "  labels = data['y']\n",
    "  data = data.drop('y', axis=1)\n",
    "  print(f'Dataset: {dataset}')\n",
    "\n",
    "  clamp = CLAMP(clusterng_algorithm=KMeans(n_clusters=cluster_number), classification_model=xgb.XGBClassifier())\n",
    "\n",
    "  clf = GridSearchCV(clamp, parameters, scoring=scorer, cv=5, refit='precision', n_jobs=1)\n",
    "  clf.fit(data)  #, labels)\n",
    "\n",
    "  cv_restuls.append(clf.cv_results_)\n",
    "  cv_datasets.append(dataset)\n",
    "  cv_clf.append(clf)\n",
    "\n",
    "with open('art_results.pickle', 'wb') as f:\n",
    "  pickle.dump([cv_restuls, cv_datasets], f)\n",
    "\n",
    "rem_hmr_files()  # remove temp hmr files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8efea6-5384-42a6-89b6-8797819bd585",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08b686-5ace-41dc-b240-261bbca470f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}