{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC now= '2022-05-14T14:23:16'\n",
      "Virtualenv used: /usr/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/mmozolewski/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mmozolewski/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/mmozolewski/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/mmozolewski/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/mmozolewski/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import sys\n",
    "\n",
    "print(f\"UTC now= '{datetime.datetime.utcnow().isoformat().split('.')[0]}'\")\n",
    "print(f'Virtualenv used: {sys.executable}')\n",
    "\n",
    "import importlib\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#for model-building\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix,roc_curve,auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_bert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "\n",
    "import spacy\n",
    "importlib.reload(spacy)\n",
    "spacy_nlp_model = spacy.load('en_core_web_lg')\n",
    "\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "# for LIME import necessary packages\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from random import randint\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def _save_pickle(df, outfile, results_folder):\n",
    "  if results_folder:\n",
    "    results_folder_local = f\"{str(Path('~').expanduser().resolve())}/{results_folder}\"\n",
    "    # results_folder_local = results_folder\n",
    "  else:\n",
    "    results_folder_local = f\"{str(Path('~').expanduser().resolve())}/data\"\n",
    "  Path(results_folder_local).mkdir(parents=True, exist_ok=True)\n",
    "  fname = f\"{results_folder_local}/{outfile}.pickle\"\n",
    "  print(fname)\n",
    "  with open(fname, 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "\n",
    "def get_LSA(df:pd.DataFrame, column_to_omit:str, variance:float):\n",
    "\n",
    "  def get_LSA_variance(how_many_dimension):\n",
    "    how_many_dimension = math.floor(how_many_dimension)\n",
    "    svd = TruncatedSVD(how_many_dimension)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "    lsa.fit_transform(_df)\n",
    "    return svd.explained_variance_ratio_.sum()*100\n",
    "\n",
    "  def get_LSA_delta(how_many_dimension):\n",
    "    explained_variance = get_LSA_variance(how_many_dimension)\n",
    "    delta = round(math.fabs(explained_variance - variance), 0)\n",
    "    # print(f\"how_many_dimension= {how_many_dimension}, explained_variance= {explained_variance}, delta= {delta}\")\n",
    "    return delta\n",
    "\n",
    "  def optimize_how_many_dimension(bounds):\n",
    "    res = minimize_scalar(get_LSA_delta, bounds=bounds, method='bounded')\n",
    "    # print(res.x)\n",
    "    how_many_dimension = math.floor(res.x)\n",
    "    if get_LSA_variance(how_many_dimension) < variance:\n",
    "      how_many_dimension += 1\n",
    "    return (how_many_dimension-1, get_LSA_variance(how_many_dimension))\n",
    "\n",
    "  _df = df.loc[:, (df.columns != column_to_omit) & (df.columns != 'cluster_kmeans')]\n",
    "  variables_max_count = len(_df.columns) - 1\n",
    "  how_many_dimension, explained_variance = optimize_how_many_dimension(bounds=(1, variables_max_count/4))\n",
    "  if explained_variance < variance:\n",
    "    print(f\"variance is to low: {explained_variance}\")\n",
    "    how_many_dimension, explained_variance = optimize_how_many_dimension(bounds=(how_many_dimension+1, variables_max_count/2))\n",
    "  if explained_variance < variance:\n",
    "    print(f\"variance is to low: {explained_variance}\")\n",
    "    how_many_dimension, explained_variance = optimize_how_many_dimension(bounds=(how_many_dimension+1, variables_max_count))\n",
    "\n",
    "  while get_LSA_variance(how_many_dimension) < variance:\n",
    "    how_many_dimension += 1\n",
    "\n",
    "  svd = TruncatedSVD(how_many_dimension)\n",
    "  normalizer = Normalizer(copy=False)\n",
    "  lsa = make_pipeline(svd, normalizer)\n",
    "  _df_lsa = pd.DataFrame(lsa.fit_transform(_df))\n",
    "  print(f\"explained variance perc = {svd.explained_variance_ratio_.sum()*100}%\")\n",
    "\n",
    "  if 'cluster_kmeans' in df.columns:\n",
    "    _df_lsa = pd.concat([df[['article_which_cities', 'cluster_kmeans']], _df_lsa], axis=1, join=\"inner\")\n",
    "  else:\n",
    "    _df_lsa = pd.concat([df[['article_which_cities']], _df_lsa], axis=1, join=\"inner\")\n",
    "  # df_edges_lsa['cluster_kmeans'] = -1\n",
    "  return _df_lsa\n",
    "\n",
    "# get_LSA(df_edges_one_hot, 'article_which_cities', 95.0)\n",
    "\n",
    "\n",
    "def k_means(df, column_to_ommit, how_many_clusters):\n",
    "  kmeans = KMeans(n_clusters=how_many_clusters, random_state=2022).fit(df.loc[:, (df.columns != column_to_ommit) & (df.columns != 'cluster_kmeans')])\n",
    "  # kmeans.labels_\n",
    "  # print(pd.DataFrame(kmeans.cluster_centers_).reset_index())\n",
    "  # print(pd.DataFrame(kmeans.cluster_centers_).reset_index()['index'])\n",
    "  df['cluster_kmeans'] = kmeans.labels_\n",
    "  # df_only_one_hots = df.loc[:, (df.columns != 'article_which_cities') & (df.columns != 'cluster_kmeans')]\n",
    "  # df[['article_which_cities', 'cluster_kmeans']]\n",
    "  return df\n",
    "\n",
    "\n",
    "def get_TSNE_and_PCA_embeddings(df_edges, column_to_ommit):\n",
    "  df_edges_independent_vars = df_edges.loc[:, (df_edges.columns != column_to_ommit) & (df_edges.columns != 'cluster_kmeans')]\n",
    "\n",
    "  df_tsne = pd.DataFrame(TSNE(n_components=2).fit_transform(df_edges_independent_vars))\n",
    "  df_tsne['cluster'] = df_edges['cluster_kmeans']\n",
    "  df_tsne.columns = ['x1', 'x2', 'cluster']\n",
    "  df_tsne['article_which_cities'] = df_edges['article_which_cities']\n",
    "\n",
    "  df_pca = pd.DataFrame(PCA(n_components=2).fit_transform(df_edges_independent_vars))\n",
    "  df_pca['cluster'] = df_edges['cluster_kmeans']\n",
    "  df_pca.columns = ['x1', 'x2', 'cluster']\n",
    "  df_pca['article_which_cities'] = df_edges['article_which_cities']\n",
    "\n",
    "  scaler = MinMaxScaler()\n",
    "  df_tsne[['x1', 'x2']] = scaler.fit_transform(df_tsne[['x1', 'x2']])\n",
    "  df_pca[['x1', 'x2']] = scaler.fit_transform(df_pca[['x1', 'x2']])\n",
    "\n",
    "  return df_tsne, df_pca\n",
    "\n",
    "\n",
    "def wrap_by_word(s, n):\n",
    "  '''returns a string where \\\\n is inserted between every n words'''\n",
    "  if isinstance(s, str):\n",
    "    a = s.split()\n",
    "    ret = ''\n",
    "    for i in range(0, len(a), n):\n",
    "      ret += ' '.join(a[i:i+n]) + '\\n'\n",
    "\n",
    "    return ret\n",
    "  return \"\"\n",
    "\n",
    "# x = wrap_by_word('There is a dog and fox fighting in the park and there is an apple falling down.', 4)\n",
    "# print(x)\n",
    "def get_article_info(df_with_article_which_cities_column, column):\n",
    "  df_nodes_cp = df_nodes.copy()\n",
    "  df_nodes_cp['article_which_cities'] = df_nodes_cp['ID']#.apply(lambda x: f\"{x:02d}\")\n",
    "  df_merged = df_with_article_which_cities_column.merge(df_nodes_cp, on='article_which_cities', how='left')\n",
    "\n",
    "  return df_merged['article_which_cities'] + \" (\" + df_merged['Year'].apply(lambda x: str(int(x)) if not np.isnan(x) else \"\") + \") \" + \": \\n\" + df_merged[column].apply(lambda x: wrap_by_word(x, 3))\n",
    "\n",
    "\n",
    "def label_point(x, y, val, ax, factor):\n",
    "  a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
    "  for i, point in a.iterrows():\n",
    "    ax.text(point['x'] + 1*factor, point['y'] - 10*factor, str(point['val']))\n",
    "\n",
    "\n",
    "def visualise_clusters(df_2d_embedding, factor=1/200, base_size=6):\n",
    "  _fig, _ax = plt.subplots(1, 1, figsize=(base_size*4,base_size*2))\n",
    "  sns.scatterplot(data=df_2d_embedding, x='x1', y='x2', hue='cluster', style='cluster', legend=\"full\", alpha=0.9, s=120, palette=\"deep\", ax=_ax)\n",
    "  label_point(df_2d_embedding['x1'], df_2d_embedding['x2'], get_article_info(df_2d_embedding, column='Title'), _ax, factor)\n",
    "  # display(_fig)\n",
    "\n",
    "# get_article_info(df_edges_lsa_tsne, column='Title')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "3387 documents\n",
      "4 categories\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\n",
    "  \"--lsa\",\n",
    "  dest=\"n_components\",\n",
    "  type=\"int\",\n",
    "  help=\"Preprocess documents with latent semantic analysis.\",\n",
    ")\n",
    "op.add_option(\n",
    "  \"--no-minibatch\",\n",
    "  action=\"store_false\",\n",
    "  dest=\"minibatch\",\n",
    "  default=True,\n",
    "  help=\"Use ordinary k-means algorithm (in batch mode).\",\n",
    ")\n",
    "op.add_option(\n",
    "  \"--no-idf\",\n",
    "  action=\"store_false\",\n",
    "  dest=\"use_idf\",\n",
    "  default=True,\n",
    "  help=\"Disable Inverse Document Frequency feature weighting.\",\n",
    ")\n",
    "op.add_option(\n",
    "  \"--use-hashing\",\n",
    "  action=\"store_true\",\n",
    "  default=False,\n",
    "  help=\"Use a hashing feature vectorizer\",\n",
    ")\n",
    "op.add_option(\n",
    "  \"--n-features\",\n",
    "  type=int,\n",
    "  default=10000,\n",
    "  help=\"Maximum number of features (dimensions) to extract from text.\",\n",
    ")\n",
    "op.add_option(\n",
    "  \"--verbose\",\n",
    "  action=\"store_true\",\n",
    "  dest=\"verbose\",\n",
    "  default=False,\n",
    "  help=\"Print progress reports inside k-means algorithm.\",\n",
    ")\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "  return not hasattr(sys.modules[\"__main__\"], \"__file__\")\n",
    "\n",
    "\n",
    "if not is_interactive():\n",
    "  op.print_help()\n",
    "  print()\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "  op.error(\"this script takes no arguments.\")\n",
    "  sys.exit(1)\n",
    "\n",
    "\n",
    "categories = [\n",
    "  \"alt.atheism\",\n",
    "  \"talk.religion.misc\",\n",
    "  \"comp.graphics\",\n",
    "  \"sci.space\",\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "  subset=\"all\", categories=categories, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 0.437999s\n",
      "n_samples: 3387, n_features: 10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "  if opts.use_idf:\n",
    "    # Perform an IDF normalization on the output of HashingVectorizer\n",
    "    hasher = HashingVectorizer(\n",
    "      n_features=opts.n_features,\n",
    "      stop_words=\"english\",\n",
    "      alternate_sign=False,\n",
    "      norm=None,\n",
    "    )\n",
    "    vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "  else:\n",
    "    vectorizer = HashingVectorizer(\n",
    "      n_features=opts.n_features,\n",
    "      stop_words=\"english\",\n",
    "      alternate_sign=False,\n",
    "      norm=\"l2\",\n",
    "    )\n",
    "else:\n",
    "  vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    max_features=opts.n_features,\n",
    "    min_df=2,\n",
    "    stop_words=\"english\",\n",
    "    use_idf=opts.use_idf,\n",
    "  )\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "if opts.n_components:\n",
    "  print(\"Performing dimensionality reduction using LSA\")\n",
    "  t0 = time()\n",
    "  # Vectorizer results are normalized, which makes KMeans behave as\n",
    "  # spherical k-means for better results. Since LSA/SVD results are\n",
    "  # not normalized, we have to redo the normalization.\n",
    "  svd = TruncatedSVD(opts.n_components)\n",
    "  normalizer = Normalizer(copy=False)\n",
    "  lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "  X = lsa.fit_transform(X)\n",
    "\n",
    "  print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "  explained_variance = svd.explained_variance_ratio_.sum()\n",
    "  print(\n",
    "    \"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100))\n",
    "  )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, init_size=1000, n_clusters=4, n_init=1,\n",
      "                verbose=False)\n",
      "done in 0.042s\n"
     ]
    }
   ],
   "source": [
    "if opts.minibatch:\n",
    "  km = MiniBatchKMeans(\n",
    "    n_clusters=true_k,\n",
    "    init=\"k-means++\",\n",
    "    n_init=1,\n",
    "    init_size=1000,\n",
    "    batch_size=1000,\n",
    "    verbose=opts.verbose,\n",
    "  )\n",
    "else:\n",
    "  km = KMeans(\n",
    "    n_clusters=true_k,\n",
    "    init=\"k-means++\",\n",
    "    max_iter=100,\n",
    "    n_init=1,\n",
    "    verbose=opts.verbose,\n",
    "  )\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.501\n",
      "Completeness: 0.565\n",
      "V-measure: 0.531\n",
      "Adjusted Rand-Index: 0.464\n",
      "Silhouette Coefficient: 0.008\n",
      "Top terms per cluster:\n",
      "Cluster 0: funded compact permission prototype forbid fastest format silicon parents arrays\n",
      "Cluster 1: neural unbelievers photoshop innocence spacelab current blackbody fastest brings ics\n",
      "Cluster 2: cd implementation lcd sigh ch981 evening imagination fewer joint unbelievers\n",
      "Cluster 3: wang unbelievers rewrite altered askew foo damage current galilean emory\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\n",
    "  \"Silhouette Coefficient: %0.3f\"\n",
    "  % metrics.silhouette_score(X, km.labels_, sample_size=1000)\n",
    ")\n",
    "\n",
    "if not opts.use_hashing:\n",
    "  print(\"Top terms per cluster:\")\n",
    "\n",
    "  if opts.n_components:\n",
    "    original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "  else:\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "  # terms = vectorizer.get_feature_names_out()\n",
    "  terms = list(vectorizer.vocabulary_.keys())\n",
    "  # print(terms)\n",
    "  for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end=\"\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "      print(\" %s\" % terms[ind], end=\"\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}